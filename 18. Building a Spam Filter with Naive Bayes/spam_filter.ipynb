{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53f732ce",
   "metadata": {},
   "source": [
    "## Spam Filter with Naive Bayes\n",
    "\n",
    "![Image](https://ampjar.com/wp-content/uploads/2019/07/HERO-avoid-spam-filters@2x.png)\n",
    "\n",
    "### Introduction\n",
    "\n",
    "To classify messages as spam or non-spam using the naive bayes algorithm, we basically need follow these steps:\n",
    "\n",
    "* Learns how humans classify messages\n",
    "* Uses that human knowledge to estimate probabilities for new messages (spam and non-spam)\n",
    "* Classifies a new message based on these probability values\n",
    "\n",
    "In this project we'll be working with the data set provided by [The UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection). You can download data set and readme download from this link and also get data set information.\n",
    "\n",
    "There are 5,572 SMS messages that are already classified by humans. Let's read it and take a quick look on the first five rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2d7221a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "classified_sms = pd.read_csv('SMSSpamCollection',\n",
    "                            sep='\\t',\n",
    "                            header=None,\n",
    "                            names=['Label', 'SMS'])\n",
    "\n",
    "classified_sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660efaa2",
   "metadata": {},
   "source": [
    "Let's find now what percentage of the messages is spam and what isn't (\"ham\" means non-spam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bc15dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13.4% spam messages and 86.6% non-spam messages in the data set\n"
     ]
    }
   ],
   "source": [
    "total_persentages = classified_sms['Label'].value_counts(normalize=True)\n",
    "\n",
    "print('''There are {:.1%} spam messages and {:.1%} non-spam messages in the data set'''\n",
    "      .format(total_persentages[1], total_persentages[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c153fa",
   "metadata": {},
   "source": [
    "### Building the filter\n",
    "\n",
    "#### Test preparation\n",
    "\n",
    "When creating software, it's a good rule to think over tests in advance. In this case we should split our data set into two parts:\n",
    "* training set (about 80% of the dataset)\n",
    "* test set (about 20% of the dataset)\n",
    "\n",
    "We'll create the spam filter using training set only. Once we have it, we'll apply the algorithm on testing set and compare  results with the human classificatioan. Our goal here is accuracy greater than **80%**.\n",
    "\n",
    "To ensure that spam and ham messages are spread properly throughout the datasets we'll randomize all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "666486ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomize whole data set\n",
    "classified_sms.sample(frac=1, random_state=1)\n",
    "\n",
    "#Index to split with\n",
    "split_on = round(len(classified_sms) * 0.8)\n",
    "train_set = classified_sms.iloc[:split_on, :].copy()\n",
    "test_set = classified_sms.iloc[split_on:, :].copy()\n",
    "\n",
    "#Reset index\n",
    "train_set.reset_index(inplace=True, drop=True)\n",
    "test_set.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24ed229",
   "metadata": {},
   "source": [
    "Let's check the spam and ham ratio in these new sets. It should be similar to what we have in the full data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f927b9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full data set: 13.4% spam messages and 86.6% non-spam messages\n",
      "Train set: 13.5% spam messages and 86.5% non-spam messages\n",
      "Test set: 13.0% spam messages and 87.0% non-spam messages\n"
     ]
    }
   ],
   "source": [
    "train_persentages = train_set['Label'].value_counts(normalize=True)\n",
    "test_persentages = test_set['Label'].value_counts(normalize=True)\n",
    "\n",
    "print('''Full data set: {:.1%} spam messages and {:.1%} non-spam messages\n",
    "Train set: {:.1%} spam messages and {:.1%} non-spam messages\n",
    "Test set: {:.1%} spam messages and {:.1%} non-spam messages'''\n",
    "      .format(total_persentages[1], total_persentages[0],\n",
    "             train_persentages[1], train_persentages[0],\n",
    "             test_persentages[1], test_persentages[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa2112a",
   "metadata": {},
   "source": [
    "Everything looks fine.\n",
    "\n",
    "#### Data cleaning\n",
    "\n",
    "We want to classify messages by the words in them so we need to convert them into sets of words. It means:\n",
    "* Remove any punctuation\n",
    "* Reduce everything to the lowercase\n",
    "* Split the message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93f80655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>[nah, i, don, t, think, he, goes, to, usf, he,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4453</th>\n",
       "      <td>ham</td>\n",
       "      <td>[i, ve, told, you, everything, will, stop, jus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4454</th>\n",
       "      <td>ham</td>\n",
       "      <td>[or, i, guess, lt, gt, min]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4455</th>\n",
       "      <td>ham</td>\n",
       "      <td>[i, m, home, ard, wat, time, will, u, reach]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4456</th>\n",
       "      <td>ham</td>\n",
       "      <td>[storming, msg, wen, u, lift, d, phne, u, say,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4457</th>\n",
       "      <td>ham</td>\n",
       "      <td>[if, you, want, to, mapquest, it, or, somethin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4458 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Label                                                SMS\n",
       "0      ham  [go, until, jurong, point, crazy, available, o...\n",
       "1      ham                     [ok, lar, joking, wif, u, oni]\n",
       "2     spam  [free, entry, in, 2, a, wkly, comp, to, win, f...\n",
       "3      ham  [u, dun, say, so, early, hor, u, c, already, t...\n",
       "4      ham  [nah, i, don, t, think, he, goes, to, usf, he,...\n",
       "...    ...                                                ...\n",
       "4453   ham  [i, ve, told, you, everything, will, stop, jus...\n",
       "4454   ham                        [or, i, guess, lt, gt, min]\n",
       "4455   ham       [i, m, home, ard, wat, time, will, u, reach]\n",
       "4456   ham  [storming, msg, wen, u, lift, d, phne, u, say,...\n",
       "4457   ham  [if, you, want, to, mapquest, it, or, somethin...\n",
       "\n",
       "[4458 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set['SMS'] = train_set['SMS'].str.replace(\n",
    "    r'\\W', ' ',regex=True).str.lower().str.split()\n",
    "\n",
    "train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b6f152",
   "metadata": {},
   "source": [
    "Now we should create a list of unique words - the **vocabulary**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bba82f6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7813"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = []\n",
    "\n",
    "for message in train_set['SMS']:\n",
    "    for word in message:\n",
    "        vocabulary.append(word)\n",
    "        \n",
    "#Remove duplicates and return the list\n",
    "vocabulary_set = set(vocabulary)\n",
    "vocabulary = list(vocabulary_set)\n",
    "\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9517a04d",
   "metadata": {},
   "source": [
    "There are 7813 words in the vocabulary!\n",
    "\n",
    "Now we can use it to count how many words are in each message. We'll use the dictionary for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39894503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kusruthi [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "shivratri [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "oru [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "aaooooright [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "laying [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "#Create dictionary with zeroes for every word from vocabulary\n",
    "word_counts_per_sms = {unique_word: [0] * len(train_set['SMS']) for unique_word in vocabulary}\n",
    "\n",
    "#Iterate through messages\n",
    "for index, sms in enumerate(train_set['SMS']):\n",
    "    for word in sms:\n",
    "        word_counts_per_sms[word][index] += 1\n",
    "\n",
    "#Test\n",
    "for word in vocabulary[:5]:\n",
    "    print(word, word_counts_per_sms[word][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afcf228",
   "metadata": {},
   "source": [
    "These results we'll combine with the trainig dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e7a9172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>kusruthi</th>\n",
       "      <th>shivratri</th>\n",
       "      <th>oru</th>\n",
       "      <th>aaooooright</th>\n",
       "      <th>laying</th>\n",
       "      <th>beth</th>\n",
       "      <th>hill</th>\n",
       "      <th>drinking</th>\n",
       "      <th>...</th>\n",
       "      <th>ntimate</th>\n",
       "      <th>musthu</th>\n",
       "      <th>associate</th>\n",
       "      <th>hotel</th>\n",
       "      <th>gives</th>\n",
       "      <th>brats</th>\n",
       "      <th>thk</th>\n",
       "      <th>workage</th>\n",
       "      <th>slowly</th>\n",
       "      <th>hor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, o...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>[nah, i, don, t, think, he, goes, to, usf, he,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7815 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS  kusruthi  \\\n",
       "0   ham  [go, until, jurong, point, crazy, available, o...         0   \n",
       "1   ham                     [ok, lar, joking, wif, u, oni]         0   \n",
       "2  spam  [free, entry, in, 2, a, wkly, comp, to, win, f...         0   \n",
       "3   ham  [u, dun, say, so, early, hor, u, c, already, t...         0   \n",
       "4   ham  [nah, i, don, t, think, he, goes, to, usf, he,...         0   \n",
       "\n",
       "   shivratri  oru  aaooooright  laying  beth  hill  drinking  ...  ntimate  \\\n",
       "0          0    0            0       0     0     0         0  ...        0   \n",
       "1          0    0            0       0     0     0         0  ...        0   \n",
       "2          0    0            0       0     0     0         0  ...        0   \n",
       "3          0    0            0       0     0     0         0  ...        0   \n",
       "4          0    0            0       0     0     0         0  ...        0   \n",
       "\n",
       "   musthu  associate  hotel  gives  brats  thk  workage  slowly  hor  \n",
       "0       0          0      0      0      0    0        0       0    0  \n",
       "1       0          0      0      0      0    0        0       0    0  \n",
       "2       0          0      0      0      0    0        0       0    0  \n",
       "3       0          0      0      0      0    0        0       0    1  \n",
       "4       0          0      0      0      0    0        0       0    0  \n",
       "\n",
       "[5 rows x 7815 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts_df = pd.DataFrame(word_counts_per_sms)\n",
    "train_set_clean = pd.concat([train_set, word_counts_df], axis=1)\n",
    "train_set_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdf7ee3",
   "metadata": {},
   "source": [
    "#### Probabilities\n",
    "\n",
    "We've got cleaned training set and we are ready to go. Let's first recall the probabilities that we need to classify new messages:\n",
    "\n",
    "\\begin{equation}\n",
    "P(Spam | w_1,w_2, ..., w_n) \\propto P(Spam) \\cdot \\prod_{i=1}^{n}P(w_i|Spam) \\\\\n",
    "P(Ham | w_1,w_2, ..., w_n) \\propto P(Ham) \\cdot \\prod_{i=1}^{n}P(w_i|Ham) \\\\\n",
    "\\end{equation}\n",
    "\n",
    "We must calculate `p_spam` and `p_ham` and also probabilities for each word by the following formulas:\n",
    "\n",
    "\\begin{equation}\n",
    "P(w_i|Spam) = \\frac{N_{w_i|Spam} + \\alpha}{N_{Spam} + \\alpha \\cdot N_{Vocabulary}} \\\\\n",
    "P(w_i|Ham) = \\frac{N_{w_i|Ham} + \\alpha}{N_{Ham} + \\alpha \\cdot N_{Vocabulary}}\n",
    "\\end{equation}\n",
    "\n",
    "Let's prepare some constant values first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89553246",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Probabilities\n",
    "p_spam = train_persentages[1]\n",
    "p_ham = train_persentages[0]\n",
    "\n",
    "#Number of words\n",
    "n_spam = train_set_clean.loc[train_set_clean['Label'] == 'spam'].sum()[2:].sum()\n",
    "n_ham = train_set_clean.loc[train_set_clean['Label'] == 'ham'].sum()[2:].sum()\n",
    "n_voc = len(vocabulary)\n",
    "\n",
    "# Laplace coef\n",
    "alpha = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd88580f",
   "metadata": {},
   "source": [
    "Now we move on probabilities themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f79619f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kusruthi 4.3189081800120926e-05 6.150250622712876e-05\n",
      "shivratri 4.3189081800120926e-05 3.075125311356438e-05\n",
      "oru 4.3189081800120926e-05 7.687813278391095e-05\n",
      "aaooooright 4.3189081800120926e-05 3.075125311356438e-05\n",
      "laying 4.3189081800120926e-05 3.075125311356438e-05\n"
     ]
    }
   ],
   "source": [
    "#Dictionaries for probabilities\n",
    "words_given_spam = {unique_word: 0 for unique_word in vocabulary}\n",
    "words_given_ham = {unique_word: 0 for unique_word in vocabulary}\n",
    "\n",
    "for word in vocabulary:\n",
    "    #How many times word appears in spam and ham messages\n",
    "    n_word_spam = train_set_clean.loc[train_set_clean['Label'] == 'spam', word].sum()\n",
    "    n_word_ham = train_set_clean.loc[train_set_clean['Label'] == 'ham', word].sum()\n",
    "    \n",
    "    #Probabilities\n",
    "    p_word_given_spam = (n_word_spam + alpha\n",
    "                        ) / (n_spam + alpha * n_voc)\n",
    "    \n",
    "    p_word_given_ham = (n_word_ham + alpha\n",
    "                        ) / (n_ham + alpha * n_voc)\n",
    "    \n",
    "    #Update dictionaries\n",
    "    words_given_spam[word] = p_word_given_spam\n",
    "    words_given_ham[word] = p_word_given_ham\n",
    "    \n",
    "    \n",
    "#Test\n",
    "for word in vocabulary[:5]:\n",
    "    print(word, words_given_spam[word], words_given_ham[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051435c0",
   "metadata": {},
   "source": [
    "#### Filter itself\n",
    "\n",
    "Once we've calculated all the parameters we need, we can create the spam filter. The filter is a function that:\n",
    "* Takes in as input a new message **(w1, w2, ..., wn)**\n",
    "* Calculates **P(Spam|w1, w2, ..., wn)** and **P(Ham|w1, w2, ..., wn)**\n",
    "* Compares the values of **P(Spam|w1, w2, ..., wn)** and **P(Ham|w1, w2, ..., wn)**, and then:\n",
    "    * If **P(Ham|w1, w2, ..., wn) > P(Spam|w1, w2, ..., wn)**, then the message is classified as **ham**\n",
    "    * If **P(Ham|w1, w2, ..., wn) < P(Spam|w1, w2, ..., wn)**, then the message is classified as **spam**\n",
    "    * If **P(Ham|w1, w2, ..., wn) = P(Spam|w1, w2, ..., wn)**, then the algorithm may request **human** help\n",
    "    \n",
    "Let's write such function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8859bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Spam|message): 7.608478331174781e-24\n",
      "P(Ham|message): 3.335898472753021e-26\n",
      "Label: Spam\n",
      "P(Spam|message): 4.546575542387177e-17\n",
      "P(Ham|message): 9.245128581483754e-12\n",
      "Label: Ham\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def classify(message):\n",
    "    #Clean message like train set\n",
    "    message = re.sub('\\W', ' ', message)\n",
    "    message = message.lower()\n",
    "    message = message.split()\n",
    "\n",
    "    #Calculate probabilities\n",
    "    p_spam_given_message = p_spam\n",
    "    p_ham_given_message = p_ham\n",
    "    \n",
    "    for word in message:\n",
    "        if word in words_given_spam:\n",
    "            p_spam_given_message *= words_given_spam[word]\n",
    "            \n",
    "        if word in words_given_ham:\n",
    "            p_ham_given_message *= words_given_ham[word]\n",
    "    \n",
    "    #Print results\n",
    "    print('P(Spam|message):', p_spam_given_message)\n",
    "    print('P(Ham|message):', p_ham_given_message)\n",
    "\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        print('Label: Ham')\n",
    "    elif p_ham_given_message < p_spam_given_message:\n",
    "        print('Label: Spam')\n",
    "    else:\n",
    "        print('Equal proabilities, have a human classify this!')\n",
    "        \n",
    "#Test function\n",
    "classify('WINNER!! Send this secret code C3421 to unlock the money!')\n",
    "classify(\"I'm on my way\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1d853d",
   "metadata": {},
   "source": [
    "#### Test the algorithm\n",
    "\n",
    "It worked with two examples, now it's time use the algorithm with the test set. Let's write similar function for this purpose.\n",
    "\n",
    "Then we'll apply it to the test set and find the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93f183c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The filter accuracy - 98.56%\n"
     ]
    }
   ],
   "source": [
    "def classify_test_set(message):\n",
    "    #Clean message like train set\n",
    "    message = re.sub('\\W', ' ', message)\n",
    "    message = message.lower()\n",
    "    message = message.split()\n",
    "\n",
    "    #Calculate probabilities\n",
    "    p_spam_given_message = p_spam\n",
    "    p_ham_given_message = p_ham\n",
    "    \n",
    "    for word in message:\n",
    "        if word in words_given_spam:\n",
    "            p_spam_given_message *= words_given_spam[word]\n",
    "            \n",
    "        if word in words_given_ham:\n",
    "            p_ham_given_message *= words_given_ham[word]\n",
    "\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        return 'ham'\n",
    "    elif p_ham_given_message < p_spam_given_message:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'need human classification'\n",
    "    \n",
    "test_set['predicted'] = test_set['SMS'].apply(classify_test_set)\n",
    "\n",
    "correct = (test_set['predicted'] == test_set['Label']).sum()\n",
    "accuracy = correct / len(test_set)\n",
    "print('The filter accuracy - {:.2%}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e080c7",
   "metadata": {},
   "source": [
    "Wow, we've got superb result - **98.56%**! I've expected that accuracy to be worse than 80% to be honest.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "We've created a very efficient spam filter using the multinomial Naive Bayes algorithm. The result accuracy is **98.56%** which is great because we initially aimed for an accuracy of 80% only.\n",
    "\n",
    "Further we could make more complex algorithm by including the letter case, for example."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
